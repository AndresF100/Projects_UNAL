{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvXur5cFbDJg"
      },
      "source": [
        "# 1. Cargue sus datos\n",
        "\n",
        "Por favor cargue los datos en la siguiente sección"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Objetivos"
      ],
      "metadata": {
        "id": "a9EhyHdIy7y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Objetivo general\n",
        "- Objetivos especificos"
      ],
      "metadata": {
        "id": "-sxy-UwAjKhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Hipótesis"
      ],
      "metadata": {
        "id": "hh1HknfvzAqg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-xktJRf60Bi"
      },
      "source": [
        "# 1.3. ¿Qué pregunta(s) quiere responder a partir de sus datos?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El proyecto de este modulo es muy amplio, debido a las diferentes arquitecturas de bases de datos presentadas y a sus deseos."
      ],
      "metadata": {
        "id": "01AahruoQJbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sqlite\n",
        "\n",
        "- Postgresql\n",
        "\n",
        "- Cassandra\n",
        "\n",
        "- MongoDB\n",
        "\n",
        "- Dask\n",
        "\n",
        "- Spark"
      ],
      "metadata": {
        "id": "PnOukijmQs-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sqlite\n",
        "---\n",
        "\n",
        "- Cargar datos de distinta naturaleza (4 o 5 tablas), con interrelaciones\n",
        "- Distintos tipos de consulta\n",
        "\n",
        "\n",
        "\n",
        "### Sencillos\n",
        "- Cargar datos\n",
        "- Análisis de datos\n",
        "- Consultas básicas\n",
        "- Consultas avanzadas\n",
        "\n",
        "### Medios\n",
        "- Cargar datos en sqlite y enviarlos a mongodb o a PySpark a un RDD\n",
        "- Realizar al menos unas 3 o 4 consultas distintas utilizando `common table expressions (CTEs) with o subqueries`\n",
        "- Crear indices en distintas tablas explicando porque son necesarios\n",
        "\n",
        "### Avanzados\n",
        "- Enviar datos a Apache Kafka y serializarlos a sqlite utilizando Python\n",
        "- Cargar datos en sqlite y hacer machine learning en Dask o PySpark utilizando archivos parquet o csv"
      ],
      "metadata": {
        "id": "QXlZ_gtAQ4ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Postgresql (postgis análisis geoespacial y qgis)\n",
        "---\n",
        "\n",
        "- Cargar datos de distinta naturaleza\n",
        "- Distintios tipos de consulta\n",
        "- Clara definición de datos\n",
        "- Resolver la hipótesis que se esta planteando\n",
        "\n",
        "## Ejemplos\n",
        "\n",
        "\n",
        "### Sencillos\n",
        "- Cargar datos\n",
        "- Análisis de datos\n",
        "- Consultas básicas\n",
        "- Consultas avanzadas\n",
        "\n",
        "### Medios\n",
        "- Cargar datos en postgresql y enviarlos a mongodb o a PySpark a un RDD\n",
        "- Cargar datos en formato JSONB en postgresql\n",
        "- Crear indices en distintas tablas explicando porque son necesarios\n",
        "- Hacer consultas en formato JSONB en postgresql\n",
        "- Realizar al menos unas 3 o 4 consultas distintas utilizando `common table expressions (CTEs)` o subqueries\n",
        "\n",
        "### Avanzados\n",
        "- Crear un ambiente principal-secundario en postgresql\n",
        "- Enviar datos a Kafka y serializarlos a postgresql utilizando Python\n",
        "- Cargar datos en Postgresql y hacer machine learning en Dask o PySpark"
      ],
      "metadata": {
        "id": "A_uWy_YHRV0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MongoDB\n",
        "---\n",
        "\n",
        "- Realizar distintos tipos de agregación\n",
        "- Realizar distintos tipos de agrupamiento\n",
        "- Plantear el modelo `query first design`\n",
        "-\n",
        "\n",
        "\n",
        "## Ejemplos\n",
        "\n",
        "### Sencillos\n",
        "- Cargar datos\n",
        "- Análisis de datos\n",
        "- Consultas básicas\n",
        "- Consultas avanzadas\n",
        "\n",
        "### Medios\n",
        "- Cargar datos en postgresql y enviarlos a mongodb\n",
        "- Cargar datos en MongoDB y procesarlos usando Dask o usando PySpark\n",
        "- Realizar al menos unas 3 o 4 consultas distintas utilizando map-reduce\n",
        "\n",
        "### Avanzados\n",
        "- Docker para hacer un ambiente distribuido\n",
        "- Con RaspberryPI crear un cluster de datos\n",
        "- Cargar datos en MongoDB y hacer machine learning en Dask o PySpark"
      ],
      "metadata": {
        "id": "Ix1lZ7paRscj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dask\n",
        "---\n",
        "\n",
        "\n",
        "## Ejemplos\n",
        "\n",
        "### Sencillos\n",
        "- Cargar datos de csv, json, xml, etc.\n",
        "- Realizar un procesamiento distribuido utilizan n-jobs\n",
        "- Crear un par de consultas que involucren particiones de datos con un grafo de al menos dos o tres niveles\n",
        "\n",
        "### Medios\n",
        "- Cargar datos en postgresql y procesarlos usando dask.\n",
        "- Cargar datos en MongoDB y procesarlos usando Dask\n",
        "- Comparar los resultados de hacer machine learning con SkLearn vs hacerlo con Dask-ML\n",
        "\n",
        "### Avanzados\n",
        "- Docker para hacer un ambiente distribuido\n",
        "- Con RaspberryPI crear un cluster de datos y analizar los datos con Dask\n",
        "- Cargar datos en MongoDB y hacer machine learning en Dask"
      ],
      "metadata": {
        "id": "K8qeR6tMUjjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark\n",
        "---\n",
        "\n",
        "\n",
        "## Ejemplos\n",
        "\n",
        "### Sencillos\n",
        "- Cargar datos de csv, json, xml, etc.\n",
        "- Realizar un procesamiento distribuido utilizan n-jobs\n",
        "- Crear un par de consultas que involucren particiones de datos\n",
        "\n",
        "### Medios\n",
        "- Cargar datos en postgresql y procesarlos usando Spark.\n",
        "- Cargar datos en MongoDB y procesarlos usando Spark.\n",
        "- Comparar los resultados de hacer machine learning con SkLearn vs hacerlo con PySpark (entender las limitaciones y usos de aprendizaje distribuido)\n",
        "\n",
        "### Avanzados\n",
        "- Docker para hacer un ambiente distribuido (super avanzado)\n",
        "- Con RaspberryPI crear un cluster de datos y analizar los datos con Dask\n",
        "- Cargar datos en MongoDB y hacer machine learning en PySpark"
      ],
      "metadata": {
        "id": "2M7KxTPZVfTV"
      }
    }
  ]
}